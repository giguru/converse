{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-retrieval Conversation Question Answering\n",
    "Based on the paper _Open-retrieval Conversation Question Answering_ by _Qu et al_.\n",
    "\n",
    "Since ConverSE is built upon Haystack. This notebook is very similar to the original notebook on Dense Passage Retrieval https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial6_Better_Retrieval_via_DPR.ipynb#scrollTo=kFwiPP60A6N7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please wait a moment while I gather a list of all available modules...\n",
      "\n",
      "IPython             builtins            jupyter_core        runpy\n",
      "PyQt5               bz2                 jupyterlab_pygments s3transfer\n",
      "__future__          cProfile            keras               sacremoses\n",
      "_abc                caffe2              keyword             sched\n",
      "_ast                calendar            langdetect          scipy\n",
      "_asyncio            certifi             lib2to3             secrets\n",
      "_bisect             cffi                libfuturize         select\n",
      "_blake2             cgi                 libpasteurize       selectors\n",
      "_bootlocale         cgitb               linecache           send2trash\n",
      "_bz2                chardet             locale              sentencepiece\n",
      "_cffi_backend       chunk               logging             seqeval\n",
      "_codecs             click               lxml                setup\n",
      "_codecs_cn          cloudpickle         lzma                setup_cython\n",
      "_codecs_hk          cmath               macpath             setuptools\n",
      "_codecs_iso2022     cmd                 mailbox             shelve\n",
      "_codecs_jp          code                mailcap             shlex\n",
      "_codecs_kr          codecs              main                shutil\n",
      "_codecs_tw          codeop              mako                signal\n",
      "_collections        collections         markupsafe          simplejson\n",
      "_collections_abc    colorsys            marshal             sip\n",
      "_compat_pickle      compileall          math                sipconfig\n",
      "_compression        concurrent          mimetypes           sipdistutils\n",
      "_contextvars        configparser        mistune             site\n",
      "_crypt              contextlib          mlflow              six\n",
      "_csv                contextvars         mmap                sklearn\n",
      "_ctypes             copy                modulefinder        smmap\n",
      "_ctypes_test        copyreg             multipart           smtpd\n",
      "_curses             coverage            multiprocessing     smtplib\n",
      "_curses_panel       crypt               nbclient            sndhdr\n",
      "_datetime           csv                 nbconvert           socket\n",
      "_decimal            ctypes              nbformat            socketserver\n",
      "_distutils_hack     curses              nest_asyncio        spwd\n",
      "_dummy_thread       cythonmagic         netrc               sqlalchemy\n",
      "_elementtree        databricks_cli      nis                 sqlalchemy_utils\n",
      "_functools          dataclasses         nntplib             sqlite3\n",
      "_hashlib            datetime            notebook            sqlparse\n",
      "_heapq              dateutil            ntpath              sre_compile\n",
      "_imp                dbm                 nturl2path          sre_constants\n",
      "_io                 decimal             numbers             sre_parse\n",
      "_json               decorator           numpy               ssl\n",
      "_locale             defusedxml          opcode              starlette\n",
      "_lsprof             difflib             operator            stat\n",
      "_lzma               dill                optparse            statistics\n",
      "_markupbase         dis                 os                  storemagic\n",
      "_md5                distlib             ossaudiodev         string\n",
      "_multibytecodec     distutils           packaging           stringprep\n",
      "_multiprocessing    docker              pandas              struct\n",
      "_opcode             docs                pandocfilters       subprocess\n",
      "_operator           doctest             parser              sunau\n",
      "_osx_support        docx                parso               symbol\n",
      "_pickle             dotmap              past                sympyprinting\n",
      "_posixsubprocess    dummy_threading     pathlib             symtable\n",
      "_py_abc             easy_install        pdb                 sys\n",
      "_pydecimal          editor              pexpect             sysconfig\n",
      "_pydev_bundle       elasticapm          pickle              syslog\n",
      "_pydev_comm         elasticsearch       pickleshare         tabnanny\n",
      "_pydev_imps         email               pickletools         tabulate\n",
      "_pydev_runfiles     encodings           pip                 tarfile\n",
      "_pydevd_bundle      ensurepip           pipes               telnetlib\n",
      "_pydevd_frame_eval  entrypoints         pkg_resources       tempfile\n",
      "_pyio               enum                pkgutil             tenacity\n",
      "_pyrsistent_version errno               platform            terminado\n",
      "_queue              faiss               plistlib            termios\n",
      "_random             farm                pluggy              test\n",
      "_remote_module_non_sriptable fastapi             poplib              testpath\n",
      "_sentencepiece      faulthandler        posix               tests\n",
      "_sha1               fcntl               posixpath           textwrap\n",
      "_sha256             filecmp             pprint              this\n",
      "_sha3               fileinput           profile             threading\n",
      "_sha512             filelock            prometheus_client   threadpoolctl\n",
      "_signal             flask               prompt_toolkit      tika\n",
      "_sitebuiltins       flask_cors          pstats              time\n",
      "_socket             flask_restplus      psutil              timeit\n",
      "_sqlite3            fnmatch             psycopg2            tkinter\n",
      "_sre                formatter           pty                 token\n",
      "_ssl                fractions           ptyprocess          tokenize\n",
      "_stat               ftplib              pvectorc            tokenizers\n",
      "_string             functools           pwd                 toml\n",
      "_strptime           future              py                  torch\n",
      "_struct             gc                  py_compile          tornado\n",
      "_symtable           genericpath         pyclbr              tox\n",
      "_sysconfigdata_aarch64_conda_cos7_linux_gnu getopt              pycompletionserver  tqdm\n",
      "_sysconfigdata_i686_conda_cos6_linux_gnu getpass             pycparser           trace\n",
      "_sysconfigdata_m_linux_x86_64-linux-gnu gettext             pydantic            traceback\n",
      "_sysconfigdata_powerpc64le_conda_cos7_linux_gnu git                 pydev_app_engine_debug_startup tracemalloc\n",
      "_sysconfigdata_x86_64_apple_darwin13_4_0 gitdb               pydev_console       traitlets\n",
      "_sysconfigdata_x86_64_conda_cos6_linux_gnu glob                pydev_coverage      transformers\n",
      "_testbuffer         grp                 pydev_ipython       tty\n",
      "_testcapi           gunicorn            pydev_jupyter_plugin turtle\n",
      "_testimportmultiple gzip                pydev_jupyter_utils turtledemo\n",
      "_testmultiphase     h11                 pydev_jupyter_vars  types\n",
      "_thread             h5py                pydev_pysrc         typing\n",
      "_threading_local    hashlib             pydev_test_pydevd_reload typing_extensions\n",
      "_tkinter            haystack            pydev_tests         unicodedata\n",
      "_tracemalloc        heapq               pydev_tests_mainloop unittest\n",
      "_warnings           hmac                pydev_tests_python  urllib\n",
      "_weakref            html                pydevconsole        urllib3\n",
      "_weakrefset         http                pydevd              uu\n",
      "_xxtestfuzz         httptools           pydevd_concurrency_analyser uuid\n",
      "abc                 idlelib             pydevd_file_utils   uvicorn\n",
      "aifc                idna                pydevd_plugins      uvloop\n",
      "alembic             imaplib             pydevd_pycharm      venv\n",
      "aniso8601           imghdr              pydevd_tracing      virtualenv\n",
      "antigravity         imp                 pydoc               warnings\n",
      "appdirs             importlib           pydoc_data          wave\n",
      "argon2              importlib_metadata  pyexpat             wcwidth\n",
      "argparse            inspect             pygments            weakref\n",
      "array               integration         pyparsing           webbrowser\n",
      "ast                 interpreterInfo     pyrsistent          webencodings\n",
      "async_generator     io                  pytz                websocket\n",
      "asynchat            ipaddress           qtconsole           werkzeug\n",
      "asyncio             ipykernel           qtpy                wheel\n",
      "asyncore            ipykernel_launcher  querystring_parser  widgetsnbextension\n",
      "atexit              ipython_genutils    queue               wsgiref\n",
      "attr                ipywidgets          quopri              xdrlib\n",
      "audioop             itertools           random              xml\n",
      "autoreload          itsdangerous        re                  xmlrpc\n",
      "backcall            jedi                readline            xxlimited\n",
      "base64              jinja2              regex               xxsubtype\n",
      "bdb                 jmespath            reprlib             yaml\n",
      "binascii            joblib              requests            zipapp\n",
      "binhex              json                resource            zipfile\n",
      "bisect              jsonschema          rest_api            zipimport\n",
      "bleach              jupyter             rlcompleter         zipp\n",
      "boto3               jupyter_client      rmagic              zlib\n",
      "botocore            jupyter_console     runfiles            zmq\n",
      "\n",
      "Enter any module name to get more help.  Or, type \"modules spam\" to search\n",
      "for modules whose name or summary contain the string \"spam\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have a GPU running\n",
    "# !nvidia-smi\n",
    "\n",
    "# !pip install git+https://github.com/deepset-ai/haystack.git # Install the latest master of Haystack\n",
    "# !pip install git+https://github.com/giguru/converse.git  # Install the latest master of Converse\n",
    "\n",
    "help(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'converse'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-6a65018855e0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mhaystack\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprint_answers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mconverse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfarm\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mFARMReader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mconverse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTransformersReader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mconverse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretriever\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense_passage_retriever\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDensePassageRetriever\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'converse'"
     ]
    }
   ],
   "source": [
    "from haystack import Finder\n",
    "from haystack.preprocessor.cleaning import clean_wiki_text\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
    "from haystack.utils import print_answers\n",
    "\n",
    "from converse.src.reader.farm import FARMReader\n",
    "from converse.src.reader.transformers import TransformersReader\n",
    "from converse.src.retriever.dense_passage_retriever import DensePassageRetriever\n",
    "from converse.src.converse import Converse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexer and data\n",
    "\n",
    "Add document collection to a DocumentStore. The original text will be indexed. Conversion into embeddings can be\n",
    "is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/01/2020 19:22:27 - INFO - faiss -   Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "# In Colab / No Docker environments: Start Elasticsearch from source\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.6.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download evaluation data, which is a subset of Natural Questions development set containing 50 documents\n",
    "doc_dir = \"../data/nq\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/nq_dev_subset_v2.json.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure these indices do not collide with existing ones, the indices will be wiped clean before data is inserted\n",
    "doc_index = \"tutorial5_docs\"\n",
    "label_index = \"tutorial5_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add document collection to a DocumentStore. The original text will be indexed. Conversion into embeddings can be\n",
    "# is done below.\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\",\n",
    "                                            create_index=False, embedding_field=\"emb\",\n",
    "                                            embedding_dim=768, excluded_meta_data=[\"emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add evaluation data to Elasticsearch Document Store\n",
    "# We first delete the custom tutorial indices to not have duplicate elements\n",
    "document_store.delete_all_documents(index=doc_index)\n",
    "document_store.delete_all_documents(index=label_index)\n",
    "document_store.add_eval_data(filename=\"../data/nq/nq_dev_subset_v2.json\", doc_index=doc_index, label_index=label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at facebook/dpr-question_encoder-single-nq-base and are newly initialized: ['question_encoder.bert_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DPRContextEncoder were not initialized from the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base and are newly initialized: ['ctx_encoder.bert_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",  # TODO replace with ORConvQA model\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",  # TODO replace with ORConvQA model\n",
    "    use_gpu=True,\n",
    "    embed_title=True,\n",
    "    max_seq_len=256,\n",
    "    batch_size=16,\n",
    "    remove_sep_tok_from_untitled_passages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed passages\n",
    "Since retrieval will be done on the embeddings, the embedding representation of the documents need to be computed\n",
    "This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/01/2020 19:23:18 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "10/01/2020 19:23:18 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "10/01/2020 19:23:22 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "10/01/2020 19:23:25 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"loss_ignore_index\": -1}\n",
      "10/01/2020 19:23:29 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "10/01/2020 19:23:29 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n",
      "10/01/2020 19:23:29 - INFO - farm.infer -    0    0    0    0    0    0    0 \n",
      "10/01/2020 19:23:29 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "10/01/2020 19:23:29 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n",
      "10/01/2020 19:23:29 - INFO - farm.infer -               \n"
     ]
    }
   ],
   "source": [
    "# Load a local model or any of the QA models on Hugging Face's model hub (https://huggingface.co/models)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Converse(reader, retrievers=[retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() missing 2 required positional arguments: 'label_index' and 'doc_index'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-851101747ed7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Evaluate combination of Reader and Retriever through Finder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfinder_eval_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfinder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtop_k_retriever\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop_k_reader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mfinder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprint_eval_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfinder_eval_results\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: eval() missing 2 required positional arguments: 'label_index' and 'doc_index'"
     ]
    }
   ],
   "source": [
    "# Evaluate combination of Reader and Retriever through Finder\n",
    "finder_eval_results = finder.eval(top_k_retriever=1, top_k_reader=10)\n",
    "finder.print_eval_results(finder_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}